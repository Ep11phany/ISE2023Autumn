# EmojiSeeker：Easy Search, Effortless Talking！

We consider this project mostly as a tool for enhancing users' interaction experience with a great variety of softwares. There are plenty of occasions when users have a demand for searching images in their own image library based on their semantic content such as searching memes when chatting in social networking softwares like Wechat or QQ, and searching specific images that was pre-captured in a present trip for posting a Moment, moments when the alternative images are quite too much for picking up the right one. Considering it's quite time-wasting for users to filter images or memes they need one by one, we think AI can help solve searching images by their semantics, because AI can assist through a technology called "content-based image retrieval" (CBIR). This technology relies on artificial intelligence (AI) algorithms to understand and analyze the visual content of images, making it easier for users to find images based on their semantic meaning rather than just textual descriptions or tags. Overall, AI facilitates semantic image search by automating the analysis of image content and understanding the meaning behind it. This leads to more efficient and intuitive image search experiences for users. Given the prevalance of emoticon-based communication among adolescents in today's internet user demographic, it can be inferred that the user base for EmojiSeeker is quite extensive.

As mentioned above, it speeds up the meta behavior of "search images by their semantic content", which we think takes effect in nearly everyone's daily lives. Considering concrete benifits of this tool, real-time capability is urgently needed in such scenarios. Try imagine that you have to search a Doraemon meme with a specific expression for 30, 40, or even much more seconds before sending it to your friend when chatting just because it hasn't been frequently used, that'll be horrible!

Our AI model will be optimized more towards reward because this task returns a set of images(can be ranked by fitness score), and user needs to choose the one that fits its demand the most; so we are more focused on whether the demanded image is included in the final set of the output rather than how much proportion of the wrong-predicted images are there in the output set.

We understand that the tradeoff for choosing this method means our model will perform not so well on the first-match rate, which means the rate of the first-ranked image satisfying the demand of the user. However, from our perspective, there'll be no difference whether the model return 1 or 10 images to the user after query, provided that the returned images include the image that users exactly want, and the size is small enough for user to choose within instant. We assume preliminarily, if the satisfaction rate of the first 5 or 10 returned images for semantic search task drops below 80% on test set, then we should fine-tune the CLIP model for better performance. 

We consider to use pre-trained CLIP model and fine-tune it to make it adapt to our goal. Based on multiple works on fine-tuning CLIP and its zero-shot ability, we believe it's capable enough for zero-shot CLIP model for our project. We plan to add human-labelled dataset aiming for specific scenarios (like memes) to enhance the model's capability. 